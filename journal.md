# Journal

Thursday 28/09/2022
- Explored how Napari works and its plugins, really just looked at the CellPose integration plugin. Still need to workout how to correct the predictions.
- Had a meeting with Ivor, Helfrid and Ryan for 1 hour where we talked about the features Ryan was collecting on cells, Ivor explained we should explore VAEs to get more features, I talked about my Napari exploration and the 2009 surface curvature histogram paper that Helfrid said was a good start, I proposed to do a Napari presenation to Helfrid's lab and Ivor said I have to make a sketch of what my dissertation will look like.
- I started the GitHub project and started setting up the project with the correct requirements, taking them from the cell-SCT project.
- I need to make a plan of what I want to achieve in the next two weeks for this project.

Monday 03/10/2022
- I ran into some issues while trying to install napari and run it on the Windows Chichester lab computers, but now it is installed and running and I documented how to do it and avoid the issues I had and how I solved them on the "setup.md" file in this project.
- I followed the tutorial from Napari to create a very basic Napari plugin and then how to do it with cookiecutter (which creates a template for you) and I documented one issue I had (not having installed git before installing cookiecutter with pip) and its solution in the "plugin_creation.md" file in this project.

Tuesday 04/10/2022
- I quickly setup Napari on a new Windows computer and made a little correction the the "quick setup" commands, adding the command that pip installs the missing "imageio_ffmpeg" package.
- I read Ivor's email on some thoughts and guidance on the group meetings he will be hosting for his group of 3rd year projects (which I am apart of). I read [this blog on training neural networks](http://karpathy.github.io/2019/04/25/recipe/) that he recommended in his email.
- I sent a ticket to the IT department (because Luke Igerson is sick these days) about getting GlobalProtect installed on every computer.
- I started looking at the available Napari plugins and will start evaluating some of them, with a note inside of this project that keeps track of my review. This will probably be when GlobalProtect will be installed so I can do a quick check with some Omero data of how they work. I will probably spend a few hours testing a few of them.
- Importantly, I need to make a Napari plugin for tomorrow that just measures the size of cells (or their mask from CellPose2 really) that I can show to Ivor and Helfrid to show what I've learned about Napari and how I will be using it. I also need to write a quick document that shows what my objectives will be for this project.

Wednesday 05/10/2022
- Today I played with Napari and trying to create a plugin. I uploaded the plugin I currently have to this GitHub project. The progress has mostly been disabling other widgets. I am trying to find the correct variables to be able to access the different layers in Napari from the plugin as that would let me access the mask generated by CellPose and start doing some tracking. I will be developing this tracking in a separate project, so without Napari, and then reintroducing it to Napari as it is too annoying having to restart it each time (even though it is fast, I need a faster response to my code). 
- I had a meeting with Ivor and Ryan, Helfrid was still stick. Most of the meeting was Ryan talking about soft labelling and Ivor giving advice. He also showed his VAE and Ivor gave some advice on improving it, especially on the loss function used (Ryan was using Binary Cross Entropy loss but since it is a categorical labelling (multi-class) Ivor was recommending Categorial Cross Entropy Loss). I just talked about my progress with discovering Napari, trying to build a plugin and promising a working demo plugin by next Wednesday.
- Ivor reminded me that I have to make a document that shows my core objectives for this project. I am thinking that it will be Napari tracking plugins (maybe only one plugin with the various tracking parts being the widgets) for the size, shape and labelling intensity and there will be an extra part on classifying. For now, I am focusing on being able to build a plugin and then I will do one or two tracking features that I will include in the plugin and evaluate them, then I will go back to the classification that will also be included in the plugin, and then will go back to develop more tracking features that Helfrid's lab's cell biologists may need.
- I will also need to work on the Napari demo for Helfrid's lab and also work on the interim report soon.

Monday 10/10/2022
- Lots of progress: I was able to add some very basic tracking features to a Napari plugin such as counting pixels in the mask (from a selected layer in a drop down menu) which can give some basic info on the size (in pixels) of cells (grouped or individually).
- I spent a lot of time trying to implement the ToolTip from various links that I found online (https://github.com/napari/napari/pull/2658) and I failed. I did learn some stuff about Napari in the process and also that ToolTip can be activated from the preferences and is the data from the status bar in the bottom left of the GUI but I don't understand how to customise it (as in put my own data in the status bar) and make the tooltip appear when the mouse hovers over the cell.
- With the failed ToolTip implementation, I am now working on creating my own. I feel like I can learn a lot doing this about creating Napari plugins. I think this will be the last thing I implement to show Helfrid and Ivor that I can make Napari plugins and can finally move on from software engineering stuff to machine learning stuff (ie implementing machine learning tracking (first making it work) and packaging trainable general models in a Napari plugin). It has to work through a QWidget not a magic widget as the mouse event listeners don't work in the magic widgets (I'm not sure why, probably a scope issue). So I need to find the coordinates of the mouse (done), then see if they correspond to a cell in the numpy array as the mouse coordinates indicate a pixel which is in the numpy array which, if it has a number other than 0, is a cell, and then display that information in a little self dissapearing pop up that I don't want to be a window but just a little text box next to the mouse. Otherwise it can be displayed in a text box with the widget buttons.
- I added my notes which includes ideas to add in this demo plugin in the project. It's called "steps in first Napari plugin". I also added a TIF image and its cellpose mask in the "Example_imgs" folder that I just created in this project to be used when working with this plugin.

Tuesday 11/10/2022
- Another full day's work but I was able to implement the tooltip! Honestly, I learned a lot about Napari and how it works, definetely not all though.
- What is left for the demo Napari plugin is for me to add the basic tracking that I had done in the "magic widget" and add it to the Qwidget as info on the side bar and some in the tooltip. I also want to add the measurement of the intensity of the marker, it would be cool to just be able to click into two points and it would create a square and return (printed probably) the average intensity in the marker, or just in the cell mask. This reminds me that I had other ideas to play with the cell mask such as singling out a cell using its mask. Anything seems possible right now.
- I am following Ian Mackie's advice of doing a little bit everyday on the 3rd year project by doing a lot everyday.

Wednesday 12/10/2022
- Continued working on the demo Napari plugin. Added some label information reading that can be viewed on the right hand side of the Napari window when hovering over a label. Also added creating a new layer which is just the labelled cell (and its surroundings as it takes for the crop coordinates the mouse coordinates) when double clicking on its mask.
- Worked a little bit on the project proposal.
- Had the weekly Wednesday meeting. I talked about my project, the project propsal I have to give back soon, the upcoming interim report, my plan (first work on cell classification, then tracking), my demo Napari plugin and how my exploration there is done for now, and Ivor gave some recommendations about just using the Omero images I can download for now as fighting with ITS is not the priority (since they haven't answered my emails about getting Omero installed in Chichester labs and apparently are swarmed right now).

Thursday 13/10/2022
- I finished and sent the first version of the project proposal to Ivor and Helfrid.
- I made the presentation (once again adapted from the JRA presenation I had to make for Ivor's PhD group) for friday morning's meeting.

Friday 14/10/2022
- We had the group meeting on Friday morning (Ivor, Mae, Ekin, Mihaela, Sergey and me). We all presented which took 1h50 (from 9 to 10:50) and we all asked questions about each others projects as Ivor gave some light feedback and questioning.

Wednesday 19/10/2022
- Made a file to connect to Omero and download the images locally from a plate ID. I downloaded the images from plate id 812 (the one that I used for the JRA) in Richmond and uploaded them to my OneDrive.
- Ivor gave some feedback monday on Slack on my project proposal and said that I should actually do the tracking before the classification as otherwise I will be doing the same as Ryan, and the tracking can actually help the classification. I agreed.
- At the meeting I caught Helfrid up on what I was doing and we discussed the new plan for the project that starts with tracking, Helfrid told me he wanted to see the demo Napari plugin next Monday at 11AM (we set the meeting together) so I want to develop it a little further, maybe add a layer creation or the start of the matching algorithm (but very basic, like just match from the centroids) on a series of micrographs.
- At the meeting Ivor recommended that, to save resources and because they are so similar in the features they look at (probably), just one CNN should segment, classify and track the cells. Therefore, I started working after the meeting on making a U-Net just to segment the cells for now, its in the u_net_exploration project on my GitHub, I made that project public. I got the inspiration to make the CNN a U-Net as that is what CellPose uses. It does not work yet!
- As soon as the meeting ended I just worked on trying to make a U-Net.

Monday 24/10/2022
- Worked on the Napari demo plugin to make the 'single out the cell by double clicking on the mask' actually work and a centroid calculator that also creates a 'points' layer that just shows the centroids of all of the cells.
- Showed the demo Napari plugin to Helfrid and he was happy. He told me more about how he would like the PCNA to be tracked over time to see if we can get some information there about the cell cycle phases. He also presented me to some members of his lab such as Ron. Also there is no Wednesday meeting as Ivor is not here and I met with Helfrid today, I should probably tell Ryan.
- I worked on the PCNA measurement python file which for now only reads the images, singles out the cells and gets their centroids. I need it to get all the images and their mask, get all of their individual cells singled out and then with their centroids do a distance comparison to match up the cells and measure their PCNA (average pixel intensity) and map the change over the 180 images.

Tuesday 25/10/2022
- Worked on the 'read_PCNA' python file to match the cells by coordinate. Right now, it goes up to finding pairs of cells that match between images. So it's not bad, just that because of something that I have just realized about the data that I will explain on the next point, we're getting weird results. But for now, it's able to take a series of images, create masks of them with CellPose, use those masks to single out the cells, get the centroids of the cells, match cells between two images according to the closest centroids, match the pairs of cells according to the matching coordinates between images.
- What I found out about the data, when I downloaded plate 812, is that plate 812 is one big image of a plate of cells that has been cut up into many bits so that it's not just one massive image. Basically, in the 180 TIF images I have, they are all pieces of a puzzle to make one single image. They are not time sequential images like I thought they were. I thought that each image in that plate was of the same cells but at different times: I was wrong, they are just pictures of differents parts of the plate. That means that I cannot do any sequential analysis since the images are not sequential i.e. they are not pictures taken at different points in time of the same cells. Therefore, I need to go back to Richmond and download the pictures taken of the plate at a later time (I'm guessing in a time of minutes or hours). That may not exist: maybe only one picture of the plate was taken and that's it. Since Helfrid's lab studies the cell cycle, I'm sure that there are pictures of the same cells taken at different time, and if plate 812 does not have that, I'll need to ask him for that data.
- My end goal with this part of the project is to be able to match the cells up at different points in time and be able to label them (with the ground truth data method that was used for the CNN during the JRA (basically using metric data from the 4 channels)) and then measuring the change in expression of the markers in the cells and see if those show a pattern with their cell cycle phase. I am hoping for that data to be indicative of a pattern that can be used to classify them. Otherwise it will be a cool thing to implement in the Napari tracking plugin. Furthermore, with the cell matching algorithm, I can also show how much a cell moves and have some cool gifs of individual cells (maybe just plotting their centroids) of them moving. Nice tracking data overall.
- After getting the data tomorrow and seeing if this works, I should also move back to the U-Net soon.

Wednesday 26/10/2022
- I found out today that GlobalProtect was added on the Chichester PCs (at least the one I am randomly using today), since the 18th of October.
- While exploring the plate 812 on Omero to find which parts were sequential (or if it was at all), I found that the pictures of the cells were taken very closely in time (less than a minute) and that the Z-axis (usually used for time) was of size 1x1 for each picture, and that the pictures don't even look sequential (the cells don't match from field to field). Therefore, I concluded that they were not sequential. I did learn about the difference between plates and wells thansk to [this link](https://docs.openmicroscopy.org/ome-model/5.5.7/developers/screen-plate-well.html). I told Helfrid and he told me to use plate ID 449 which has sequential images of cells and are marked with mCherry (which I belive is a PCNA marker just like ALEXA555).
- I tried downloading the data of plate ID 449 from the same computer in the Chichester lab but was unable to as the connection kept failing. I made a Python file called 'omero_download' that should connect, but it does not because of an SSL handshake error. My solution is the following: I'll go to Richmond tomorrow morning and download the data from plate ID 449 locally from there.
- My plan right now is to get some interesting sequential tracking data (just matching the cells, measuring their change in PCNA, measuring their change in size) to show Helfrid that it is progressing. If I can do that by tomorrow that would be great, and then I will continue on the U-Net.
- Remember: rename the omero_download file to omero_download.py (maybe its the missing .py extension that is causing that fail in connection?).

Thursday 27/10/2022
- Continued working on the omero_download file: now has the .py extension, now has 3 functions that can be used to read data from Omero, download that data locally and visualise that data in sequential manner with a slider. I had some problems with SSL but were solved by moving some files into the DLL folder (found two useful stackoverflow answers: [this one](https://stackoverflow.com/questions/42563757/conda-update-condahttperror-http-none/60342954#60342954) and [this one](https://stackoverflow.com/questions/60750197/pywin32-importerror-dll-load-failed-the-specified-module-could-not-be-found)). I then downloaded the 60 series of each 192 images to my OneDrive, ready to be downloaded and studied.
- I went back to the Chichester labs, found out that for each series of images it was the same pictures each time. I tried connecting to Omero from the Chichester lab computer but kept running into issues with SSL and Omero. So I went back to Richmond labs and corrected the code (I used the iterator from the outer loop into the inner loop when trying to reference the individual images) and I downloaded for the first two series, I will download more at a later point when I have time, for now just two series is enough I think. Let's get some PCNA tracking done.
- I was able to look at the tracking methods with our now sequential data. They are correct, even the plotting that was done at the bottom actually shows that the cells do match to the ones close to the other one according to their centroid. The only issue is that, even though it works well, and might maybe be good information to keep right now is that the all_pairs that is returned from the "match images" returns duplicates as it returns the matching pairs. Therefore I just need to reinvestigate how the code works, which I belive is just matching if the right value of the first pair matches with the left value of the second pair, to stop making it add duplicates. Otherwise, I need to make sure that in all of this centroid matching, the mask is kept as I now need the mask to extract data from the image such as the averaged out intensity to measure the change in PCNA sequentially. Very exciting stuff! So tomorrow (or next time I work on this, maybe next week), I first need to figure out how I keep the mask tied to the centroid matching so that it can be retrieved. I could assign an index to the centroids such as '0:12' to indicate that it is from the first image and is the 12th mask. To figure out.

Friday 28/10/2022
- Figuring out the cell matching functions and redefining it to be clearer as it needs to carry the data forward. Since the arrays keep the same index positions for the cell data (mostly need their images and masks), it is understandable. Not done yet.

Monday 31/10/2022
- Working on interim report, made the sections and a few notes for most of them. Got to 871 words. Tomorrow should be the big day for writing everything out, should reach 3000 words. I will make most of the diagrams on Wednesday I think.

Thursday 24/11/2022
- Worked on the U-Net and got it to distill CellPose quite efficiently.

Friday 25/11/2022
- Group meeting with Ivor and his other 3rd year project students where he gave me some advice on the U-Net such as using random crops and switching to residual crops.
- Individual meeting with Helfrid where he gave me some ideas about skipping the classification and new data that was coming in. We talked about integrating the tracking directly into the U-Net and predicting other markers.
- I worked on getting random crops from 10 to 250 pixels to train the U-Net with (all padded to 250x250). I started remaking the U-Net from the architecture I was using for the full image (the day before) but ran into some errors with the number of channels. I might start it from scratch again.

Thursday 01/12/2022
- Worked on the U-Net in a notebook called u_net_exp. I was able to make the entire model work on smaller crops. I only trained it on 67 images so the results were not very good, I have a lot of data to train it with so I just need to launch this at some point. I need to make the model overfit on the training data, but with the weird loss its not easy to see, I need to make it predict and look at the quantitative results. The loss is enormously negative. I want the model to segment the individual cells, right now it just has one class. I also then want to implement this into Napari where you just need to make a selection on an image of cells and this model makes a mask prediction directly there. I also need to follow the blog that Ivor had recommened for the correct careful steps to take to build an ML model. I also need to use Weights&Biases to test out the correct parameters. I also need to change the loss to not be Binary Cross Entropy because I want it to be multi class, with the number of classes being unknown since it depends on the image of cells, each cell corresponds to one class.
- I just trained it for 13 epochs on 6000 images and the predicted mask seems to be very blurry and rotated 90 degrees counterclockwise.
- I should really have this journal be in its own GitHub project since this project is separated into two GitHub projects right now (this one and the U-Net exploration one).

Tuesday 06/12/2022
- Worked on the U-Net's loss. It's a binary image that it needs to predict but I went through many losses before deciding to just implement my own dice loss. Right now it predicts 1s everywhere as the punishment (loss) is the same for doing that than for not doing it which explains that it does find the cell since it thinks "if I label everything as the cell than I am bound to find it, rather than looking more accurately for it which will give me the same punishment if I miss anyway, so might as well guess everything". So I need to find a way to punish it for its amount of guesses. Maybe I could make it that the number of pixels it is allowed to label as 1 is equal to the amount of pixels that are labelled 1 on the ground-truth image, and if it labels more than that it gets an even bigger increase in loss, like a penalty for going over. That should force it to be careful. But it might end up not labeling anything because it's "too dangerous given the loss that could be incured". I need to test this out and look for a balance. This is fun.
- I am meeting Helfrid on Thursday morning. I hope to show him an implementation of the model and maybe already some predictions of markers. I need to work on that tomorrow.

Wednesday 07/12/2022
- Worked on the unet and dice, tversky and focal-tversky losses are what work best but predicting too many false positives. Sent a message to Ivor describing the issue and will see later about this in case he recognises the problem.
- Next I want to try the prediction of markers from other markers to show to Helfrid if we can get it, should not take too long to do, just need to go into the cell matching algorithm and extract the cell's average intensity on different channels. Then I want to implement the U-Net into Napari to be able to just take a crop of an image I drag-and-drop into Napari and make it segment out that part of the image to find the cells and generate the masks from then on their, basically just get the cell masks of a crop of a microscopy picture I define by clicking.

Thursday 08/12/2022
- Worked on the unet loss. Tried the dice+BCE loss and that is what works best for now, not perfect but close to getting good masks. I just need to try diceTimesBCE to see what that does. That is what I will be showing tomorrow morning at the meeting.

Sunday 15/01/2023
- Worked on the unet, started a new py file called "unet_clean.py" to not work from a notebook and not have all the kept unused code the notebook ("u_net_exp.ipynb") I was using had. But I need to keep this notebook because it has a lot of code of loss functions adapted to this model even though it still produces blank data.

Monday 16/01/2023
- Had a meeting with Helfrid and Ivor to discuss where the model currently was at. I told Ivor I would ask him for some code review on my unet soon. Helfrid told me about some new cell images coming in that are easier to classify in their cell cycle phases now. I told them that I am taking two more weeks to work on the unet then starting to work on tracking.

Monday 23/01/2023
- Uploaded the code of the u_net_exp.py, u_net_clean.py files and the 20 first masks of plate 812 micrographs to GitHub.

Tuesday 24/01/2023
- Found out that it's the sigmoid activation layer then the 0.5 thresholding that is making me have only blank results. In a segmentation model, there is no need to have an activation layer. The preditctions from the model are between 0 and 0.4 and sometimes negative but never really far from each other when comparing what is labelled as the background and what is labelled as the cells, still they can be seen visually in the predictions.
- In the group meeting I explained where I currently was and Ivor recommended that I just compare the results I get to the results to before the activation layer in Cellpose. Cellpose is in PyTorch so I should be able to access the model's products by calling its layers, that is my next step.
- Also, Ivor would like for me to present how Cellpose's model works for next week's meeting, so I have to work on that. I have to explore Cellpose's instance segmentation, how does it separate the cells to give a segmentation for each of them not just binary.
- There were also other tips to improve the model that Ivor gave me that I have in my notes from the last meeting in december that I will have to implement. I also need to start doing instance segmentation not just binary segmentation, so that has to be implemented into the model, whether it is Machine Learning or an algorithm I do not know yet.

Wednesday 25/01/22
- Explored the CellPose code and found out more about how the model works. Mainly, by just using flows[0][2] I can get the cell probabilities which is what I am now using as groundtruth for the unet. As for the last activation layer, they call it a sigmoid but I was not able to find it in the code and they say the values are between -6 and +6 and they then use a threshold above it.
- As for the instance segmentation, they seem to be using a center for the ROIs and just for each binary 1 pixel in the image seeing which center it is closest to so its just an algorithm I can study and reimplement from their code.
- For now, the unet is printing NaNs for the training and testing losses which I believe is because I am back on the computer lab GPUs and it is not detaching the tensor values and I am unsure how to do it. I will need to fix that as I want to see if the losses are decreasing or not now as they were not when I was just trying to make it predict the binary ground truth images, even though it was not making blank predictions anymore. Then, I will want to test the model on a dice metric and IoU metric to measure how well it does against other models. Then I need to implement the skip layers and the other tips from Ivor. Finally, the model can go through weights and biases. After that, its time for instance segmentation to get the individual cells!
- Also, when using cellpose with a GPU it needs to be written in the model that GPU is True, it does not just detect that PyTorch has the GPU connected.
